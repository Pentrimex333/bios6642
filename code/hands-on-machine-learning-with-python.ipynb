{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading/preparation\n",
    "\n",
    "Recommend you use [Pandas](https://pandas.pydata.org/)\n",
    "\n",
    "Other options:\n",
    "* [NumPy](https://www.numpy.org/)\n",
    "\n",
    "Scikit learn includes a number of publicly available datasets that can be used for learning ML. From the documentation:\n",
    "\n",
    "***A dataset is a dictionary-like object that holds all the data and some metadata about the data. This data is stored in the `.data` member, which is a `n_samples, n_features` array. In the case of supervised problem, one or more response variables are stored in the `.target` member. More details on the different datasets can be found in the dedicated section.***\n",
    "\n",
    "Some of the steps involved:\n",
    "* Removing erroneous data\n",
    "* Correcting errors\n",
    "* Extracting parts of a corpus of data with automated tools.\n",
    "* Integrating data from various sources\n",
    "* Feature engineering/data enrichment\n",
    "* Semantic mapping\n",
    "\n",
    "**NOTE:** Most machine learning models/functions in Scikit expect data to be normalized (mean centered and scaled by the standard deviation times n_samples). Tree based methods do not usually require this.\n",
    "\n",
    "These steps are often repeated multiple times as a project progresses - data visualization and modeling often result in more data preparation.\n",
    "\n",
    "Data Cleaning takes 50 - 90% of a data scientists time:\n",
    "* https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fgilpress%2Ffiles%2F2016%2F03%2FTime-1200x511.jpg\n",
    "* https://dataconomy.com/2016/03/why-your-datascientist-isnt-being-more-inventive/\n",
    "\n",
    "For more instruction, seee this excellent tutorial showing some examples of data loading, preparation, and cleaning: https://pythonprogramming.net/machine-learning-tutorial-python-introduction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import  some of the libraries that we'll need\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for the Diabetes dataset is available at: https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset\n",
    "\n",
    "Columns in the dataset:\n",
    "* Age\n",
    "* Sex\n",
    "* Body mass index\n",
    "* Average blood pressure\n",
    "* S1\n",
    "* S2\n",
    "* S3\n",
    "* S4\n",
    "* S5\n",
    "* S6\n",
    "\n",
    "**Each of these 10 feature variables have been mean centered and scaled by the standard deviation times n_samples (i.e. the sum of squares of each column totals 1).**\n",
    "\n",
    "Target:\n",
    "* A quantitative measure of disease progression one year after baseline\n",
    "\n",
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "with np.printoptions(linewidth=130):\n",
    "    print('Data - first 5\\n', diabetes.data[0:5,:])\n",
    "    print('Target - first 5\\n', diabetes.target[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=diabetes.data, columns=['age', 'sex', 'bmi', 'abp', 's1', 's2', 's3', 's4', 's5', 's6'])\n",
    "df['target'] = diabetes.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load human readable version of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare original data set to see what data looks like in native format\n",
    "url=\"https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt\"\n",
    "df=pd.read_csv(url, sep='\\t')\n",
    "# change column names to lowercase for easier reference\n",
    "df.columns = [x.lower() for x in df.columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization/exploration\n",
    "\n",
    "Recommend you start with [Seaborn](http://seaborn.pydata.org/) - Makes matplotlib easier; can access any part of matplotlib if necessary. Other recommendations include:\n",
    "\n",
    "* [matplotlib](https://matplotlib.org/) One of the older and more widespread in use\n",
    "* [Altair](https://altair-viz.github.io/)\n",
    "* [Bokeh](https://bokeh.pydata.org/en/latest/)\n",
    "* [Plot.ly](https://plot.ly/python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style(\"ticks\", {\n",
    "    'axes.grid': True,\n",
    "    'grid.color': '.9',\n",
    "    'grid.linestyle': u'-',\n",
    "    'figure.facecolor': 'white', # axes\n",
    "})\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=df.age, y=df.y, hue=df.sex, palette='Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=df.age, y=df.bmi, hue=df.sex, palette='Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=df.age, y=df.bmi, kind='hex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = df[df.sex == 1]\n",
    "sns.jointplot(x=tdf.age, y=tdf.bmi, kind='hex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = df[df.sex == 2]\n",
    "sns.jointplot(x=tdf.age, y=tdf.bmi, kind='hex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.y, rug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"sex\", palette='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the matplotlib extension for interactivity\n",
    "\n",
    "This will affect all subsequent plots, regardless of cell location.\n",
    "\n",
    "Best to run this before any plotting in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=df.age, y=df.bmi, hue=df.sex, palette='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "\n",
    "Recommend you use [scikit-learn](https://scikit-learn.org/stable/)\n",
    "\n",
    "Deep Learning options:\n",
    "\n",
    "* [Caffe](http://caffe.berkeleyvision.org/)\n",
    "* [Fastai](https://docs.fast.ai/) - Simplifies deep learning similar to scikit-learn; based on PyTorch\n",
    "* [Keras](https://keras.io/)\n",
    "* [PyTorch](https://pytorch.org/)\n",
    "* [TensorFlow](https://www.tensorflow.org/overview/)\n",
    "\n",
    "Natural Language Processing options:\n",
    "\n",
    "* [nltk](http://www.nltk.org/)\n",
    "* [spaCy](https://spacy.io/)\n",
    "* [Stanford NLP Libraries](https://nlp.stanford.edu/software/)\n",
    "\n",
    "Computer Vision:\n",
    "* [OpenCV](https://opencv.org/)\n",
    "\n",
    "Forecasting/Time Series:\n",
    "\n",
    "* [Prophet](https://facebook.github.io/prophet/)\n",
    "* [statsmodels](https://www.statsmodels.org/stable/index.html) - Also does other statistical techniques and machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, model_selection, svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create linear regression object\n",
    "regr = LinearRegression()\n",
    "\n",
    "# by convention, X is features and y is target\n",
    "# random_state: Set a number here to allow for same results each time\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see documentation on `train_test_split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??model_selection.train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/26319259/sci-kit-and-regression-summary\n",
    "import sklearn.metrics as metrics\n",
    "def regression_results(y_true, y_pred):\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))\n",
    "    \n",
    "regression_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `explained_variance` of `0.455` means that approximately 45% of the variance in the Target variable is explained by the linear regression formula\n",
    "\n",
    "### Support Vector Machine Regression\n",
    "\n",
    "The objective of this algorithm is to maximize the distance between the decision boundary and the samples that are closest to the decision boundary. Decision boundary is called the “Maximum Margin Hyperplane.” Samples that are closest to the decision boundary are the support vectors. Through mapping of the various dimensions of data (n) into higher dimensional space via a kernel function e.g. k(x,y) each individual maybe separated from its neighbor to better identify those classified into each category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Support Vector Machine regression object\n",
    "svm_regr = svm.SVR(gamma='auto')\n",
    "\n",
    "# Train the model using the training sets\n",
    "svm_regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = svm_regr.predict(X_test)\n",
    "\n",
    "regression_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regression\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is an algorithm that a few years ago was considered state of the art for applied machine learning and Kaggle competitions when dealing with structured data.\n",
    "\n",
    "XGBoost is an implementation of gradient boosted decision trees designed for speed and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy.stats as st\n",
    "\n",
    "one_to_left = st.beta(10, 1)\n",
    "from_zero_positive = st.expon(0, 50)\n",
    "\n",
    "params = {  \n",
    "    'n_estimators': st.randint(3, 40),\n",
    "    'max_depth': st.randint(3, 40),\n",
    "    'learning_rate': st.uniform(0.05, 0.4),\n",
    "    'colsample_bytree': one_to_left,\n",
    "    'subsample': one_to_left,\n",
    "    'gamma': st.uniform(0, 10),\n",
    "    'reg_alpha': from_zero_positive,\n",
    "    'min_child_weight': from_zero_positive,\n",
    "    'objective': ['reg:squarederror']\n",
    "}\n",
    "\n",
    "xgbreg = XGBRegressor(nthreads=-1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = RandomizedSearchCV(xgbreg, params, n_jobs=1, cv=5, iid=False)  \n",
    "gs.fit(X_train, y_train)\n",
    "gs_pred = gs.predict(X_test)\n",
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results(y_test, gs_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "As we want to demonstrate classification (Target values are part of a class, not continuous numbers) we will switch to a different dataset. See https://scikit-learn.org/stable/datasets/index.html#breast-cancer-wisconsin-diagnostic-dataset for details.\n",
    "\n",
    "Attribute Information:\n",
    " \t\n",
    "* radius (mean of distances from center to points on the perimeter)\n",
    "* texture (standard deviation of gray-scale values)\n",
    "* perimeter\n",
    "* area\n",
    "* smoothness (local variation in radius lengths)\n",
    "* compactness (perimeter^2 / area - 1.0)\n",
    "* concavity (severity of concave portions of the contour)\n",
    "* concave points (number of concave portions of the contour)\n",
    "* symmetry\n",
    "* fractal dimension (“coastline approximation” - 1)\n",
    "\n",
    "Class/Target:\n",
    "* WDBC-Malignant\n",
    "* WDBC-Benign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = datasets.load_breast_cancer()\n",
    "\n",
    "with np.printoptions(linewidth=160):\n",
    "    print('Data - first 5\\n', bc.data[0:5,:])\n",
    "    print('Target - first 5\\n', bc.target[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by convention, X is features and y is target\n",
    "# random_state: Set a number here to allow for same results each time\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(bc.data, bc.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Support Vector Machine Classifier object\n",
    "svmc = svm.SVC(kernel='linear', gamma='auto')\n",
    "\n",
    "# Train the model using the training sets\n",
    "svmc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = svmc.predict(X_test)\n",
    "\n",
    "svmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (svmc, metrics.classification_report(y_test, y_pred)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'y_pred': y_pred,\n",
    "        'y_test':    y_test\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data, columns=['y_test','y_pred'])\n",
    "confusion_matrix = pd.crosstab(df['y_test'], df['y_pred'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "sns.heatmap(confusion_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "xclas = XGBClassifier()\n",
    "xclas.fit(X_train, y_train) \n",
    "xg_y_pred = xclas.predict(X_test)\n",
    "\n",
    "cross_val_score(xclas, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (xclas, metrics.classification_report(y_test, xg_y_pred)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, xg_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering (unlabeled data)\n",
    "\n",
    "Principle Component Analysis (PCA) is a technique used to emphasize variation and bring out strong patterns in a dataset. It's often used to make data easy to explore and visualize as you can use it to find those variables that are most unique and just keep 2 or 3 which can then be easily visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "\n",
    "n_components = 2\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=10)\n",
    "X_ipca = ipca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if plot data in 2 dimensions, are there any obvious clusters?\n",
    "sns.scatterplot(x=X_ipca[:, 0], y=X_ipca[:, 1], palette='Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we label data by Target variable?\n",
    "sns.scatterplot(x=X_ipca[y == 0, 0], y=X_ipca[y == 0, 1], palette='Set1')\n",
    "sns.scatterplot(x=X_ipca[y == 1, 0], y=X_ipca[y == 1, 1], palette='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means clustering\n",
    "\n",
    "This technique requires you to know the number of clusters when you start. Since you may not know the number of clusters, you can visually determine the number based on distortion. See https://towardsdatascience.com/k-means-clustering-with-scikit-learn-6b47a369a83c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# calculate distortion for a range of number of cluster\n",
    "distortions = []\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(\n",
    "        n_clusters=i, init='random',\n",
    "        n_init=10, max_iter=300,\n",
    "        tol=1e-04, random_state=0\n",
    "    )\n",
    "    km.fit(X)\n",
    "    distortions.append(km.inertia_)\n",
    "\n",
    "# plot\n",
    "plt.plot(range(1, 11), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=2,\n",
    "    init='random',\n",
    "    n_init=10,\n",
    "    max_iter=300, \n",
    "    tol=1e-04,\n",
    "    random_state=0\n",
    ")\n",
    "y_km = km.fit_predict(bc.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 3 clusters\n",
    "plt.scatter(\n",
    "    bc.data[y_km == 0, 0], bc.data[y_km == 0, 1],\n",
    "    s=50, c='lightgreen',\n",
    "    marker='s', edgecolor='black',\n",
    "    label='cluster 1'\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    bc.data[y_km == 1, 0], bc.data[y_km == 1, 1],\n",
    "    s=50, c='orange',\n",
    "    marker='o', edgecolor='black',\n",
    "    label='cluster 2'\n",
    ")\n",
    "\n",
    "# plot the centroids\n",
    "plt.scatter(\n",
    "    km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n",
    "    s=250, marker='*',\n",
    "    c='red', edgecolor='black',\n",
    "    label='centroids'\n",
    ")\n",
    "plt.legend(scatterpoints=1)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding/Explaining the model\n",
    "\n",
    "See:\n",
    "\n",
    "* LIME (Local Interpretable Model-agnostic Explanations)\n",
    "  * Github: https://github.com/marcotcr/lime \n",
    "  * Paper: https://arxiv.org/abs/1602.04938\n",
    "* SHAP (SHapley Additive exPlanations)\n",
    "  * Github: https://github.com/slundberg/shap\n",
    "  * Paper: http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Deep Learning with structured data\n",
    "\n",
    "Using Fastai Library and the Diabetes data set used for regression examples.\n",
    "\n",
    "https://www.kaggle.com/magiclantern/deep-learning-structured-data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
